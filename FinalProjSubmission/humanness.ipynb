{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2549fb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sys import stderr\n",
    "\n",
    "# if not os.path.exists(os.path.join(os.getcwd(), \"./.env\")):\n",
    "#     raise FileNotFoundError(f'Environment variable file at {os.path.join(os.getcwd(), \".env\")} not found')\n",
    "# else:\n",
    "#     %reload_ext dotenv\n",
    "#     %dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f783d761",
   "metadata": {},
   "outputs": [],
   "source": [
    "haskell_dataset = []\n",
    "haskell_filenames = []\n",
    "for file in os.listdir('./HS_Dataset/'):\n",
    "    haskell_filenames.append(file)\n",
    "    with open('./HS_Dataset/' + file, 'r') as f:\n",
    "        haskell_dataset.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "710d4ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(defun compress (x)\n",
      "  (if (consp x) \n",
      "      (compr (car x) 1 (cdr x))\n",
      "      x))\n",
      "\n",
      "(defun compr (elt n lst)\n",
      "  (if (null lst)\n",
      "      (list (n-elts elt n))\n",
      "      (let ((next (car lst)))\n",
      "        (if (eql next elt)\n",
      "            (compr elt (+ n 1) (cdr lst))\n",
      "            (cons (n-elts elt n)\n",
      "                  (compr next 1 (cdr lst)))))))\n",
      "\n",
      "(defun n-elts (elt n)\n",
      "  (if (> n 1)\n",
      "      (list n elt)\n",
      "      elt))\n",
      "      \n",
      "(print (compress '(1 1 1 0 1 0 0 0 0 1)))\n"
     ]
    }
   ],
   "source": [
    "with open('dataset.lisp', 'r') as f:\n",
    "    text = f.read()\n",
    "    snippets = text.split('##\\n')\n",
    "    lisp_dataset = [snip.strip() for snip in snippets]\n",
    "\n",
    "print(lisp_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b05d6e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "lisp_prompt_template = \\\n",
    "'''You are an agent tasked with translating Lisp code to idiomatic C code.\n",
    "\n",
    "Here is an example of what you will output, make sure to follow the format exactly:\n",
    "\n",
    "Lisp Input:\n",
    "(defun add (a b)\n",
    "  (+ a b))\n",
    "(print (add 1 2))\n",
    "(print (add 5 4))\n",
    "(print (add 6 0))\n",
    "\n",
    "C Output:\n",
    "int add(int a, int b){{\n",
    "  return a + b;\n",
    "}}\n",
    "int main(){{\n",
    "  printf(add(1, 2));\n",
    "  printf(add(5, 4));\n",
    "  printf(add(6, 0));\n",
    "}}\n",
    "\n",
    "Keep in mind:\n",
    "    - The function calls in the must be valid and syntactically correct so that the user can directly execute them. This is very important, do NOT use pseudo-code or undefined functions.\n",
    "    - All functions must be properly defined in the C output\n",
    "    - The C output must be functionally equivalent to the Lisp input such that when both programs are executed they print EXACTLY the same output\n",
    "    - Do not output anything else\n",
    "\n",
    "Final Reminder: The C code MUST be immediately executable in C\n",
    "\n",
    "Now try for yourself:\n",
    "Lisp Input:\n",
    "{0}\n",
    "\n",
    "C Output:\n",
    "'''\n",
    "\n",
    "haskell_prompt_template = \\\n",
    "r'''You are an agent tasked with translating Haskell code to idiomatic C++ code.\n",
    "\n",
    "Here is an example of what you will output, make sure to follow the format exactly:\n",
    "\n",
    "Haskell Input:\n",
    "import Text.Printf\n",
    "\n",
    "add :: Int -> Int -> Int\n",
    "add x y = x + y\n",
    "\n",
    "main = do\n",
    "    printf \"%d\\n\" $ add 1 2\n",
    "    printf \"%d\\n\" $ add 5 4\n",
    "    printf \"%d\\n\" $ add 6 0\n",
    "\n",
    "C++ Output:\n",
    "int add(int a, int b){{\n",
    "  return a + b;\n",
    "}}\n",
    "int main(){{\n",
    "  std::cout << add(1, 2) << std::endl;\n",
    "  std::cout << add(5, 4) << std::endl;\n",
    "  std::cout << add(6, 0) << std::endl;\n",
    "}}\n",
    "\n",
    "Keep in mind:\n",
    "    - The function calls in the must be valid and syntactically correct so that the user can directly execute them. This is very important, do NOT use pseudo-code or undefined functions.\n",
    "    - All functions must be properly defined in the C++ output\n",
    "    - The C++ output must be functionally equivalent to the Haskell input such that when both programs are executed they print EXACTLY the same output\n",
    "    - Do not output anything else\n",
    "\n",
    "Final Reminder: The C++ code MUST be immediately executable in C++\n",
    "\n",
    "Now try for yourself:\n",
    "Haskell Input:\n",
    "{0}\n",
    "\n",
    "C++ Output:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "665cfb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.12.1: Fast Qwen2 patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA A2. Num GPUs = 1. Max memory: 14.642 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"Qwen/Qwen2.5-Coder-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46f3a029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"qwen2.5\",\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17bd9c8",
   "metadata": {},
   "source": [
    "# Translating Haskell: use this (following) cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "546c28b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test case 0\n",
      "test case 1\n",
      "test case 2\n",
      "test case 3\n",
      "test case 4\n",
      "test case 5\n",
      "test case 6\n",
      "test case 7\n",
      "test case 8\n",
      "test case 9\n",
      "test case 10\n",
      "test case 11\n",
      "test case 12\n",
      "test case 13\n",
      "test case 14\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": haskell_prompt_template.format(test)} for test in haskell_dataset\n",
    "]\n",
    "\n",
    "for i, message in enumerate(messages):\n",
    "    print(f'test case {i}')\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        [message],\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    output = model.generate(input_ids = inputs, streamer = None, max_new_tokens = 2048,\n",
    "                    use_cache = True, temperature = 0.5, min_p = 0.1)\n",
    "\n",
    "    with open('./HS_Output/' + haskell_filenames[i][:-3] + '.cpp', 'w') as f:\n",
    "        f.write(tokenizer.batch_decode(output)[0].split('C++ Output')[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5d02d3",
   "metadata": {},
   "source": [
    "# Translating Lisp: Use this (following) cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3842ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test case 0\n",
      "test case 1\n",
      "test case 2\n",
      "test case 3\n",
      "test case 4\n",
      "test case 5\n",
      "test case 6\n",
      "test case 7\n",
      "test case 8\n",
      "test case 9\n",
      "test case 10\n",
      "test case 11\n",
      "test case 12\n",
      "test case 13\n",
      "test case 14\n",
      "test case 15\n",
      "test case 16\n",
      "test case 17\n",
      "test case 18\n",
      "test case 19\n",
      "test case 20\n",
      "test case 21\n",
      "test case 22\n",
      "test case 23\n",
      "test case 24\n",
      "test case 25\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": lisp_prompt_template.format(test)} for test in lisp_dataset\n",
    "]\n",
    "\n",
    "lisp_outputs = []\n",
    "\n",
    "for i, message in enumerate(messages):\n",
    "    print(f'test case {i}')\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        [message],\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    output = model.generate(input_ids = inputs, streamer = None, max_new_tokens = 2048,\n",
    "                    use_cache = True, temperature = 0.5, min_p = 0.1)\n",
    "\n",
    "    output_code = tokenizer.batch_decode(output)[0].split('C Output')[2]\n",
    "    # output_code = output_code[output_code.index(\"```c\")+1:] + '\\n\\n'\n",
    "    lisp_outputs.append(output_code)\n",
    "    # print(lisp_outputs[-1])\n",
    "\n",
    "\n",
    "with open('./Lisp_Output/output.c', 'w') as f:\n",
    "    f.write('\\n\\n'.join(lisp_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e86a1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.555556\n",
      "1    1.000000\n",
      "2    0.600000\n",
      "3    1.000000\n",
      "4    1.083333\n",
      "Name: 1, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "categories_from = ['Recursion', 'Recursive Lists', 'Immutable Data', 'Higher-order Functions', 'Pattern Matching']\n",
    "categories_to = ['Iteration', 'Arrays', 'State Update', 'Callbacks/Function Pointers', 'Conditional Branching']\n",
    "\n",
    "hs_data = pd.read_csv('./hs_data.csv', sep='\\t', header=None)\n",
    "# hs_data\n",
    "\n",
    "scores_by_category = np.zeros(5)\n",
    "applications_by_category = np.zeros(5)\n",
    "\n",
    "for i in range(0, hs_data.shape[0] - 1, 2):\n",
    "    from_row = hs_data.iloc[i]\n",
    "    to_row = hs_data.iloc[i+1]\n",
    "\n",
    "    scores_by_category += to_row\n",
    "    applications_by_category += from_row\n",
    "\n",
    "scores_by_category /= applications_by_category\n",
    "\n",
    "print(scores_by_category)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
